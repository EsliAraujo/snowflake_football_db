{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "import configparser\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snowpark_session():\n",
    "    \"\"\"\"Returns an snowpark session object\"\"\"\n",
    "\n",
    "    ##Get configs\n",
    "    config = configparser.ConfigParser()\n",
    "    conf_path = os.path.join('C:\\\\Users\\\\Esli\\\\.snowsql','config')\n",
    "    config.read(conf_path)\n",
    "\n",
    "    #Snowflake config\n",
    "    sfAccount = config['connections.dev']['accountname']\n",
    "    sfUser = config['connections.dev']['username']\n",
    "    sfPass = config['connections.dev']['password']\n",
    "    SfRole = config['connections.dev']['rolename']\n",
    "    sfWarehouse = config['connections.dev']['warehousename']\n",
    "    sfDatabase = config['connections.dev']['dbname']\n",
    "    CONNECTION_PARAMETERS = {\n",
    "        \"account\": sfAccount,\n",
    "        \"user\": sfUser,\n",
    "        \"password\": sfPass,\n",
    "        \"database\": sfDatabase,\n",
    "        \"warehouse\": sfWarehouse,\n",
    "        \"role\": SfRole\n",
    "    }\n",
    "\n",
    "    return Session.builder.configs(CONNECTION_PARAMETERS).create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_schema(schema_name: str):\n",
    "    \"\"\"Set working schema on Snowflake\"\"\"\n",
    "    return session.sql(f'USE SCHEMA {schema_name}').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_stage(stage_name: str, csv_schema: StructType, csv_pattern: str=None):\n",
    "    \"\"\"\n",
    "    Read the CSV's from given stage and returns as DF, if csv_pattern is not given will read all CSV's from stage.\n",
    "\n",
    "    Params:\n",
    "    stage_name (string): The name of the Stage where the files are on snowflake\n",
    "    csv_schema (StructType): The schema of the file that will be read\n",
    "    csv_pattern (String) [optional]: The regex expression that will match the files on Stage\n",
    "\n",
    "    code snippet:\n",
    "    df = read_csv_from_stage(stage_name = 'RAW_DATA_STAGE', csv_schema=csv_schema, csv_pattern='.*yds_data*[.]csv')    \n",
    "    \"\"\"    \n",
    "\n",
    "    if csv_pattern is not None:\n",
    "        stage_files = session.sql(f\"LIST @{stage_name} PATTERN='{csv_pattern}'\")\n",
    "    else:\n",
    "        stage_files = session.sql(f\"LIST @{stage_name}\")\n",
    "\n",
    "    for row in stage_files.collect():\n",
    "        csv_path = '@'+row[\"name\"]\n",
    "        df_csv = session.read.schema(csv_schema).option(\"skip_header\", 1).option(\"field_optionally_enclosed_by\",'\"').csv(csv_path)     \n",
    "        df_csv = df_csv.withColumn('path', F.lit(row[\"name\"]))\n",
    "        df_csv = df_csv.withColumn('load_at', F.lit(datetime.now()))\n",
    "        df_csv = df_csv.unionByName(df_csv)\n",
    "\n",
    "    return df_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hz_dim(table_name: str, sql_query_new: str, sql_query_append: str):\n",
    "    \"\"\"\n",
    "    Create a new dimension on HZ, or append data if it already exists\n",
    "    \n",
    "    params:\n",
    "    table_name: the name of the new dimension\n",
    "    sql_query_new: the query that will be used to read from pz table and create new table\n",
    "    sql_query_append: the query that will read pz to append on hz    \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        session.sql(f\"select * from hz_clear_strategy.{table_name} limit 1\").show()\n",
    "        print(f'Table: {table_name} alread exists')\n",
    "\n",
    "        query_dim = f\"\"\"\n",
    "                with w_new_values as ({sql_query_append})\n",
    "\n",
    "                select\n",
    "                    seq_{table_name}.NEXTVAL,\n",
    "                    w.*\n",
    "                from w_new_values w\n",
    "            \"\"\"\n",
    "\n",
    "        df_dim = session.sql(query_dim)\n",
    "        df_dim.write.mode(\"append\").saveAsTable(table_name)\n",
    "        return print(f'Execution finished, {table_name} appended')\n",
    "\n",
    "    except:\n",
    "        print(f'Table: {table_name} will be created')\n",
    "        session.sql(f'CREATE SEQUENCE IF NOT EXISTS hz_clear_strategy.seq_{table_name} START 1 INCREMENT 1;').show()\n",
    "\n",
    "        query_dim = f\"\"\"\n",
    "            select seq_{table_name}.NEXTVAL as id_{table_name},\n",
    "            t.*\n",
    "        from ({sql_query_new}) t\n",
    "        \"\"\"\n",
    "\n",
    "        df_dim = session.sql(query_dim)\n",
    "        df_dim.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "        return print(f'Execution finished, {table_name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field_list(df, control_fields: list):\n",
    "    \"\"\"\n",
    "    Receive a dataframe object and a list of control fields, return all fields from dataframe except the control fields as a string\n",
    "\n",
    "    Params:\n",
    "    df (DataFrame): The dataframe object\n",
    "    control_fields: a list of fields\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    field_list = [field for field in df.columns if field not in control_fields]\n",
    "    str_field_list = ', '.join(field_list)\n",
    "    return str_field_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_recent_value(zone: str, table_name: str, str_group_fields: str):\n",
    "    \"\"\"\n",
    "    Return the most recent value from a table considering the \"load_at\" field value and fields to group.\n",
    "\n",
    "    Params:\n",
    "    zone (string): The name of the zone of the table, iex: hz_clear_strategy\n",
    "    table_name: the name of the table\n",
    "    str_group_field: an string with the cols that should be considered when grouping to find the max(load_at), iex: 'match_id, shot_id'\n",
    "    \"\"\"\n",
    "    sql_query = f\"\"\"\n",
    "        select * from (\n",
    "            select \n",
    "                d.*,\n",
    "                max(d.load_at) over (partition by '{str_group_fields}') as max_load_at\n",
    "            from {zone}.{table_name} d\n",
    "        ) t\n",
    "        where t.max_load_at = t.load_at    \n",
    "    \"\"\"\n",
    "\n",
    "    df = session.sql(sql_query).drop('max_load_at')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################\n",
      "Available packages:\n",
      "from snowflake.snowpark.session import Session\n",
      "from snowflake.snowpark import functions as F\n",
      "from snowflake.snowpark.types import *\n",
      "from snowflake.snowpark.functions import udtf\n",
      "\n",
      "import configparser\n",
      "import os\n",
      "\n",
      "####################################################\n",
      "\n",
      "Available functions:\n",
      "get_snowpark_session\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''####################################################\n",
    "Available packages:\n",
    "    from snowflake.snowpark.session import Session\n",
    "    from snowflake.snowpark import functions as F\n",
    "    from snowflake.snowpark.types import *\n",
    "    from datetime import datetime\n",
    "\n",
    "    import configparser\n",
    "    import os\n",
    "\n",
    "####################################################\n",
    "\n",
    "Available functions, for details use help(<function_name>:\n",
    "    get_snowpark_session\n",
    "    set_schema\n",
    "    read_csv_from_stage\n",
    "    create_hz_dim\n",
    "    get_field_list\n",
    "    get_most_recent_value\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowflake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
